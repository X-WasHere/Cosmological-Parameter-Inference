{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CAMEL Multifield Dataset\n",
        "\n",
        "Implementing model suggested in 2021 paper titled \"The CAMEL Multifield Dataset: Learning the Universe's Fundamental Parameters with Artificial Intelligence\"."
      ],
      "metadata": {
        "id": "RD-2BdkAWJny"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KlZtZ6a5WF5b"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "from matplotlib.colors import LogNorm  # plotting on log scale\n",
        "\n",
        "# import tensorflow\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import RandomFlip, RandomRotation\n",
        "from keras import layers, models\n",
        "\n",
        "# import sklean\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "54qdvSXIWaAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Map:\n",
        "    '''Takes label and image file names, opens and processes the data ready to\n",
        "    implement into an model. Provides frequency plotting methods.'''\n",
        "\n",
        "    path = \"/content/drive/MyDrive/Colab_Notebooks/Camel_Project/Camels/\"\n",
        "\n",
        "    def __init__(self, labelfile: str, imagefile: str) -> None:\n",
        "        self.labelfile = labelfile\n",
        "        self.imagefile = imagefile\n",
        "\n",
        "        # load files\n",
        "        try:\n",
        "            self.params = np.loadtxt(Map.path+self.labelfile)\n",
        "            self.img_map = np.load(Map.path+self.imagefile)\n",
        "        except Exception as e:\n",
        "            print(f'Error loading files: {e}')\n",
        "            self.params = None\n",
        "            self.img_map = None\n",
        "\n",
        "        # Useful Variables\n",
        "        # images per parameter line\n",
        "        if self.img_map is not None:\n",
        "            self.img_per_param_line = self.img_map.shape[0] // self.params.shape[0]\n",
        "        # range of values varied in the dataset\n",
        "        if self.params is not None:\n",
        "            self.p_range = self.params.shape[0] // self.params.shape[1]\n",
        "\n",
        "    def info(self) -> None:\n",
        "        '''Provides some basic information about the 2DMap provided'''\n",
        "        print(\"params.shape\",self.params.shape)\n",
        "        print(\"img_map.shape\",self.img_map.shape)\n",
        "        print(\"Maps/Images per paramater line\",self.img_map.shape[0]//self.params.shape[0])\n",
        "        print(\"Maximum Value\",np.max(self.img_map[0]))\n",
        "        print(\"Minimum Value\",np.min(self.img_map[0]))\n",
        "        fig, ax = plt.subplots()\n",
        "        ax.imshow(np.log10(self.img_map[0]),cmap=\"binary\")\n",
        "        ax.set_title(\"First image\")\n",
        "\n",
        "    def log_scale(self) -> None:\n",
        "        '''Log scales and then noramlizes values ~{0,1} on per image basis to deal with\n",
        "        large intensity distribution.\n",
        "        NOTE: After experimenting this produced highest contrast and best utilization of range'''\n",
        "        # loop over every image\n",
        "        for i, image in enumerate(self.img_map):\n",
        "            self.img_map[i] = np.log10(self.img_map[i]) # log scale\n",
        "            self.img_map[i] = self.img_map[i] / np.min(self.img_map[i]) # normalize\n",
        "\n",
        "        print(\"Total max\",np.max(self.img_map))\n",
        "        print(\"Total mean\",np.mean(self.img_map))\n",
        "        print(\"Total std\",np.std(self.img_map))\n",
        "        print(\"Total min\",np.min(self.img_map),\"\\n\")\n",
        "\n",
        "    def resize(self, verbose=True) -> np.ndarray:\n",
        "        '''Resized the label array to fit images per parameter line'''\n",
        "        # resize labels to match images\n",
        "        resized_labels = np.zeros((self.img_map.shape[0], self.params.shape[1]))\n",
        "        count = 0\n",
        "        for row in self.params:\n",
        "            for i in range(self.img_per_param_line):\n",
        "                resized_labels[count] = row\n",
        "                count += 1\n",
        "        # check\n",
        "        for index, row in enumerate(self.params):\n",
        "            if np.array_equal(resized_labels[index*self.img_per_param_line], row):\n",
        "                continue\n",
        "            else:\n",
        "                print(\"ERROR : rows not equal\")\n",
        "                print(resized_labels[index*self.img_per_param_line], row)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"resized_labels.shape : {resized_labels.shape} \\n\")\n",
        "\n",
        "        return resized_labels\n",
        "\n",
        "    def extract_param(self, p: int, labels: np.ndarray, verbose=True) -> np.ndarray:\n",
        "        '''Returns a modified array that selects a specific parameter from the label file.\n",
        "        Also appropriately modifies the images array to match.\n",
        "        NOTE: Parameters must be indexed from zero.'''\n",
        "\n",
        "        # extract labels\n",
        "        new_labels = labels[(p*self.p_range*self.img_per_param_line) : ((p+1)*self.p_range*self.img_per_param_line) : 1 , p:p+1]\n",
        "\n",
        "        # modified images array\n",
        "        images = self.img_map[(p*self.p_range*self.img_per_param_line) : ((p+1)*self.p_range*self.img_per_param_line) : 1]\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"labels.shape : {new_labels.shape}\")\n",
        "            print(f\"images.shape : {images.shape}\")\n",
        "            print(f\"p_range : {self.p_range} \\n\")\n",
        "\n",
        "        return new_labels, images\n",
        "\n",
        "    def split_data(self, labels: np.ndarray, images: np.ndarray) -> np.ndarray:\n",
        "        '''Uses sklearn shuffle to mix up the labels and images in the same way - then performs an 80-10-10 split'''\n",
        "\n",
        "        labels_shuffled, images_shuffled = shuffle(labels, images) # shuffle before we split\n",
        "\n",
        "        # split into test and train\n",
        "        X_train, X_test, y_train, y_test = train_test_split(images_shuffled, labels_shuffled, test_size=0.10, random_state=42)\n",
        "        # split train further into validation and train\n",
        "        X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.11, random_state=42)\n",
        "\n",
        "        # print shapes of model input data\n",
        "        print(f\"X_train.shape :  {X_train.shape}\")\n",
        "        print(f\"X_test.shape :  {X_test.shape}\")\n",
        "        print(f\"X_val.shape: {X_val.shape}\")\n",
        "        print(f\"y_train.shape : {y_train.shape}\")\n",
        "        print(f\"y_test.shape : {y_test.shape}\")\n",
        "        print(f\"y_val.shape : {y_val.shape} \\n\")\n",
        "\n",
        "        return X_train, X_test, X_val, y_train, y_test, y_val\n",
        "\n",
        "    def plot_hist(self, labels: np.ndarray) -> None:\n",
        "        '''Plots a histogram to see if the distribution of parameter values is uniform\n",
        "        (important to prevent overfitting).'''\n",
        "\n",
        "        parameter = labels[:, 0]\n",
        "        # plot histogram\n",
        "        fig, ax = plt.subplots()\n",
        "        ax.hist(parameter[0:self.p_range*self.img_per_param_line:1], bins=self.p_range)\n",
        "        # formatting\n",
        "        ax.set_title('Frequency plot of select parameter')\n",
        "        ax.set_xlabel('Parameter')\n",
        "        ax.set_ylabel('Frequency')\n",
        "\n",
        "    @staticmethod\n",
        "    def augment_images(images: np.ndarray, labels: np.ndarray, factor: int = 2, example: bool = False) -> np.ndarray:\n",
        "        '''Implements image augmentation increasing the dataset by input variable \"factor\". Also resizes the labels\n",
        "        array to match.'''\n",
        "\n",
        "        # Implement image augmentation using albumentations\n",
        "        augmentation_pipeline = A.Compose([\n",
        "            A.HorizontalFlip(p=0.5),  # horizontal flip\n",
        "            A.VerticalFlip(p=0.5),  # vertical flip\n",
        "            A.Rotate(limit=20, p=0.5),  # rotate by up to 20 degrees\n",
        "            A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),  # random brightness and contrast\n",
        "            A.RandomGamma(gamma_limit=(80, 120), p=0.5),  # gamma correction\n",
        "            A.RandomSizedCrop(min_max_height=(50, 100), height=images.shape[1], width=images.shape[2], p=0.5),  # crop with random size\n",
        "            A.GaussianBlur(p=0.3),  # add Gaussian blur\n",
        "            A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=10, p=0.5)  # shift, scale, and rotate\n",
        "        ], p=1)\n",
        "\n",
        "        # error checking\n",
        "        try:\n",
        "            assert factor > 1\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\")\n",
        "            print(\"WARNING: factor cannot be < 2\")\n",
        "\n",
        "        # preallocate memory for new arrays\n",
        "        new_images = np.zeros(shape=(images.shape[0] * factor, images.shape[1], images.shape[2]))\n",
        "        new_labels = np.zeros(shape=(labels.shape[0] * factor, labels.shape[1]))\n",
        "\n",
        "        # augment images\n",
        "        for f in range(factor):\n",
        "            offset = f * images.shape[0]\n",
        "            for i in range(images.shape[0]):\n",
        "                image = images[i]  # retrieve image\n",
        "                label = labels[i]  # retrieve label\n",
        "\n",
        "                # apply augmentations\n",
        "                augmented = augmentation_pipeline(image=image)\n",
        "                augmented_image = augmented[\"image\"]\n",
        "\n",
        "                # add to new arrays\n",
        "                new_images[i+offset] = augmented_image\n",
        "                new_labels[i+offset] = label\n",
        "\n",
        "        # output shapes\n",
        "        print(f\"new_images.shape : {new_images.shape}\")\n",
        "        print(f\"new_labels.shape : {new_labels.shape}\")\n",
        "\n",
        "        # 3x3 grid of augmentation examples\n",
        "        if example:\n",
        "            test_image = images[0]  # test image to augment\n",
        "            # Plot augmented images\n",
        "            plt.figure(figsize=(10, 10))\n",
        "            plt.title(\"Examples of image augmentation\")\n",
        "            plt.axis(\"off\")\n",
        "            for i in range(9):\n",
        "                augmented_image = augmentation_pipeline(image=test_image)[\"image\"]\n",
        "                ax = plt.subplot(3, 3, i + 1)\n",
        "                ax.imshow(np.log10(augmented_image), cmap=\"binary\")\n",
        "                ax.axis(\"off\")\n",
        "\n",
        "        return new_images, new_labels"
      ],
      "metadata": {
        "id": "IfAgzT43Wb_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def conv_block(model, filter_size: int, H: int):\n",
        "    '''Convolutional block used in paper_model. Contains repeated\n",
        "    convolutional and batch normalization layers.'''\n",
        "\n",
        "    # first layer\n",
        "    model.add(layers.Conv2D(filters=(filter_size*H), kernel_size=(3,3), strides=(1,1), padding='same'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "    # second layer\n",
        "    model.add(layers.Conv2D(filters=(filter_size*H), kernel_size=(3,3), strides=(1,1), padding='same'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "    # third layer\n",
        "    model.add(layers.Conv2D(filters=(filter_size*H), kernel_size=(2,2), strides=(2,2), padding='valid'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "def paper_model(input_shape, H, C=1, dropout_rate=0.5):\n",
        "    '''CNN model based on reference paper\n",
        "\n",
        "    Inputs:\n",
        "    input_shape: tuple containing input shape\n",
        "    H: Hyperparameter determining number of channels in convolutional layers\n",
        "    C: int representing number of channels (>1 in case of multifield)\n",
        "    dropout_rate: droupout rate'''\n",
        "\n",
        "    model = models.Sequential()\n",
        "\n",
        "    # first block - input\n",
        "    model.add(layers.Conv2D(filters=2*H, kernel_size=(3, 3), strides=(1,1), padding='same', input_shape=input_shape))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Conv2D(filters=2*H, kernel_size=(3, 3), strides=(1,1), padding='same'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Conv2D(filters=2*H, kernel_size=(2, 2), strides=(2,2), padding='valid'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    # second block\n",
        "    conv_block(model, filter_size=4, H=H)\n",
        "    # third block\n",
        "    conv_block(model, filter_size=8, H=H)\n",
        "    # fourth block\n",
        "    conv_block(model, filter_size=16, H=H)\n",
        "    # fifth block\n",
        "    conv_block(model, filter_size=32, H=H)\n",
        "    # sixth block\n",
        "    conv_block(model, filter_size=64, H=H)\n",
        "\n",
        "    # seventh block\n",
        "    model.add(layers.Conv2D(filters=128*H, kernel_size=(4,4), strides=(1,1), padding='valid'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    # eighth block - flatten and fully connected layers\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dropout(dropout_rate))\n",
        "    model.add(layers.Dense(64*H))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(dropout_rate))\n",
        "    model.add(layers.Dense(12*H))\n",
        "\n",
        "    # ninth block - output layer\n",
        "    model.add(layers.Dense(1))\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "z0uJVkCgWgMq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}